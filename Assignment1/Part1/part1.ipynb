# -*- coding: utf-8 -*-
"""
Created on Sat Feb  2 13:10:44 2019

@author: gurjaspal
"""

# -*- coding: utf-8 -*-
"""Assignment1_practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d0sQAkFCUpvkDH0ZOuwvfPaCJt0iV2xc
"""
#%%
import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import numpy as np
from torch.autograd import Variable
import time
import matplotlib.pyplot as plt
import torch.nn as nn
print('Torch', torch.__version__, 'CUDA', torch.version.cuda)
print('Device:', torch.device('cuda:0'))
print(torch.cuda.is_available())

#%%
BATCH= 200
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((0.1307,), (0.3081,))
])

train_set = torchvision.datasets.MNIST(
    'mnist_data',
    transform=transform,
    train= True, 
    download=True)

train_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH)

test_set = torchvision.datasets.MNIST(
    'mnist_data',
    transform=transform,
    train= False, 
    download=True)

test_loader = torch.utils.data.DataLoader(test_set,batch_size=BATCH)
len(test_loader)

#%%
first_iter = next(iter(train_set))
image , label = first_iter
print(image.shape)
plt.imshow(image.squeeze(), cmap="gray")


#%%
class NeuralNet(nn.Module):
  def __init__(self):
    super(NeuralNet, self).__init__()
    self.input_layer = nn.Linear(28*28, 1024)
    self.hidden_layer1 = nn.Linear(1024, 1024)
    self.hidden_layer2 = nn.Linear(1024, 1024)
    self.hidden_layer3 = nn.Linear(1024, 1024)
    self.hidden_layer4 = nn.Linear(1024, 1024)
    self.hidden_layer5 = nn.Linear(1024, 1024)
    self.output_layer = nn.Linear(1024, 10)
    
    self.activation = nn.ReLU()

  def forward(self, data):

    data = data.view(-1, 28*28).cuda()
    output = self.activation(self.input_layer(data))
    output = self.activation(self.hidden_layer1(output))
    output = self.activation(self.hidden_layer2(output))
    output = self.activation(self.hidden_layer3(output))
    output = self.activation(self.hidden_layer4(output))
    output = self.activation(self.hidden_layer5(output))
    final_output = self.output_layer(output)
#     print(final_output.shape)
    return torch.nn.functional.log_softmax(final_output, dim=1)


#%%
neural_network = NeuralNet().cuda()
loss_function = nn.CrossEntropyLoss()
para = neural_network.parameters()
optimizer = torch.optim.Adam(params=para, lr=0.01)

for i, data in enumerate(train_loader):
  images, labels = data
  optimizer.zero_grad()
  outputs = neural_network(images.cuda())
  loss_f = loss_function(outputs.cuda(), labels.cuda())
  loss_f.backward()
  optimizer.step()

#%%
correct  = 0
total = 0
save_one = 0
with torch.no_grad():
  for data in test_loader:
    images, labels = data
    images.cuda()
    labels.cuda()
    outputs = neural_network(images)
    _, predicted = torch.max(outputs.data.cuda(), 1)
    _.cuda()
    predicted.cuda()
    total += labels.size(0)
    correct += (predicted.cuda() == labels.cuda()).sum().item()
print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))

#%%
#Part 3
thousand_test_set = torch.utils.data.Subset(test_set, 1000)
print(thousand_test_set)

#%%
